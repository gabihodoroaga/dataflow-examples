package s3tobigquery

import com.amazonaws.auth.AWSStaticCredentialsProvider
import com.amazonaws.auth.BasicAWSCredentials
import com.amazonaws.services.s3.AmazonS3ClientBuilder
import com.github.doyaaaaaken.kotlincsv.dsl.csvReader
import com.google.api.services.bigquery.model.TableRow
import com.google.api.services.bigquery.model.TimePartitioning
import com.google.cloud.secretmanager.v1.SecretManagerServiceClient
import com.google.gson.Gson
import java.io.InputStream
import java.io.Serializable
import java.nio.channels.Channels
import java.nio.channels.ReadableByteChannel
import java.time.Instant
import java.time.ZoneId
import java.time.ZoneOffset
import java.time.format.DateTimeFormatter
import java.time.temporal.ChronoUnit
import java.util.UUID
import kotlin.coroutines.*
import kotlin.system.measureTimeMillis
import org.apache.beam.runners.dataflow.util.DataflowTemplateJob
import org.apache.beam.sdk.Pipeline
import org.apache.beam.sdk.coders.StringUtf8Coder
import org.apache.beam.sdk.io.Compression
import org.apache.beam.sdk.io.FileIO
import org.apache.beam.sdk.io.FileSystems
import org.apache.beam.sdk.io.aws.options.S3ClientBuilderFactory
import org.apache.beam.sdk.io.aws.options.S3Options
import org.apache.beam.sdk.io.aws.s3.*
import org.apache.beam.sdk.io.fs.EmptyMatchTreatment
import org.apache.beam.sdk.io.fs.MatchResult
import org.apache.beam.sdk.io.fs.MetadataCoderV2
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO
import org.apache.beam.sdk.metrics.Metrics
import org.apache.beam.sdk.options.Description
import org.apache.beam.sdk.options.PipelineOptionsFactory
import org.apache.beam.sdk.options.Validation
import org.apache.beam.sdk.options.ValueProvider
import org.apache.beam.sdk.transforms.Create
import org.apache.beam.sdk.transforms.DoFn
import org.apache.beam.sdk.transforms.GroupByKey
import org.apache.beam.sdk.transforms.MapElements
import org.apache.beam.sdk.transforms.PTransform
import org.apache.beam.sdk.transforms.ParDo
import org.apache.beam.sdk.transforms.SimpleFunction
import org.apache.beam.sdk.values.KV
import org.apache.beam.sdk.values.PBegin
import org.apache.beam.sdk.values.PCollection
import org.apache.beam.sdk.values.TupleTag
import org.apache.beam.sdk.values.TupleTagList
import org.slf4j.LoggerFactory

interface CustomPipelineOptions : S3Options {
    @get:Description("The awsAccessKeyId") var awsAccessKeyId: ValueProvider<String>

    @get:Description("The awsSecretKey") var awsSecretKey: ValueProvider<String>

    @get:Description(
            "The s3/gs source bucket, in the form 's3://bucket_name' or 'gs://bucket_name'."
    )
    @get:Validation.Required
    var sourceBucket: ValueProvider<String>

    @get:Description("Start date in ISO format in UTC. Example: 2021-09-23T16:00:00Z")
    @get:Validation.Required
    var startDate: ValueProvider<String>

    @get:Description("The number of hours to look back. Example: 8 = 8 hours")
    @get:Validation.Required
    var lookBack: ValueProvider<Long>

    @get:Description(
            "BigQuery table to write to, in the form 'project:dataset.table' or 'dataset.table'."
    )
    @get:Validation.Required
    var outputTable: ValueProvider<String>

    @get:Description(
            "The build number generated by the deploy script to keep track of the version deployed in production"
    )
    var buildNumber: String
}

val TABLE_ROW_TAG = object : TupleTag<TableRow>() {}
val FILE_TAG = object : TupleTag<String>() {}

class ReadCustomCsv : DoFn<MatchResult.Metadata, TableRow>() {

    companion object {
        val GSON = Gson()
        val LOGGER = LoggerFactory.getLogger("s3tobigquery.ReadCustomCsv")

        val openFileErrors = Metrics.counter(ReadCustomCsv::class.java, "openFileErrors")
        val filesProcessed = Metrics.counter(ReadCustomCsv::class.java, "filesProcessed")
        val fileProcessingTime =
                Metrics.distribution(ReadCustomCsv::class.java, "fileProcessingTime")
        val invalidPayload = Metrics.counter(ReadCustomCsv::class.java, "invalidPayload")
    }

    @ProcessElement
    fun processElement(@Element element: MatchResult.Metadata, multiReceiver: MultiOutputReceiver) {
        val receiver = multiReceiver.get(TABLE_ROW_TAG)
        val fileReceiver = multiReceiver.get(FILE_TAG)

        val fileName = element.resourceId()
        val fileUUID = UUID.randomUUID().toString()
        filesProcessed.inc()
        fileReceiver.output(fileName.toString())

        LOGGER.info("begin process file: {}, uuid: {}", fileName, fileUUID)
        val elapsed = measureTimeMillis {
            var readableByteChannel: ReadableByteChannel? = null
            try {

                readableByteChannel = Compression.GZIP.readDecompressed(FileSystems.open(fileName))

                val stream: InputStream = Channels.newInputStream(readableByteChannel)
                csvReader().open(stream) {
                    readAllAsSequence().forEach { row: List<String> ->
                        LOGGER.info("row {}", row)
                        if (row.size == 0) {
                            return@forEach
                        }
                        if (row.size != 2) {
                            LOGGER.error(
                                    "invalid number of fields found, stop processing, file: {}",
                                    fileName
                            )
                            invalidPayload.inc()
                            return@forEach
                        }

                        receiver.output(
                                TableRow()
                                        .set("uuid", fileUUID)
                                        .set(
                                                "receiveTime",
                                                element.lastModifiedMillis().toDouble() / 1000
                                        )
                                        .set("col1", row[0])
                                        .set("col2", row[1])
                        )
                    }
                }
            } catch (e: Exception) {
                LOGGER.error("open file error, file {}", fileName, e)
                openFileErrors.inc()
            } finally {
                if (readableByteChannel != null) {
                    readableByteChannel.close()
                }
            }
        }
        fileProcessingTime.update(elapsed)
        LOGGER.info("end process file: {}, uuid: {}, time: {}", fileName, fileUUID, elapsed)
    }
}

class FolderListValueProvider(
        val bucket: ValueProvider<String>,
        val startDate: ValueProvider<String>,
        val lookBack: ValueProvider<Long>,
) : ValueProvider<List<String>>, Serializable {

    companion object {
        val LOGGER = LoggerFactory.getLogger("s3tobigquery.FolderListValueProvider")
    }

    override fun get(): List<String> {

        val sourceBucket = bucket.get()

        val startDate = Instant.from(DateTimeFormatter.ISO_INSTANT.parse(startDate.get()))
        val hours = lookBack.get()

        var formatter =
                DateTimeFormatter.ofPattern("yyyy/MM/dd/HH").withZone(ZoneId.from(ZoneOffset.UTC))

        val folders =
                (0..hours).map {
                    "$sourceBucket/${formatter.format(startDate.minus(it, ChronoUnit.HOURS))}/_SUCCESS"
                }

        LOGGER.info("begin search in folders: {}", folders)
        return folders
    }

    override fun isAccessible(): Boolean {
        return bucket.isAccessible() && startDate.isAccessible() && lookBack.isAccessible()
    }
}

class CreateFolderList(val provider: ValueProvider<List<String>>) :
        PTransform<PBegin, PCollection<String>>() {
    override fun expand(input: PBegin): PCollection<String> {
        if (provider.isAccessible()) {
            val values = Create.of(provider.get())
            return input.apply(values.withCoder(StringUtf8Coder.of()))
        }
        return input.apply(Create.of(null as Void?))
                .apply(
                        ParDo.of(
                                object : DoFn<Void?, String>() {
                                    @ProcessElement
                                    fun processElement(context: ProcessContext) {
                                        provider.get().forEach { context.output(it) }
                                    }
                                }
                        )
                )
                .setCoder(StringUtf8Coder.of())
    }
}

class GetCsvFiles(val folderOptions: FolderListValueProvider) :
        PTransform<PBegin, PCollection<MatchResult.Metadata>>() {
    override fun expand(input: PBegin): PCollection<MatchResult.Metadata> {

        return input.apply("CreateFolderList", CreateFolderList(folderOptions))
                .apply(
                        "ValidateFolder",
                        FileIO.matchAll().withEmptyMatchTreatment(EmptyMatchTreatment.ALLOW)
                )
                .apply(
                        "ExtractFolderName",
                        MapElements.via(
                                object : SimpleFunction<MatchResult.Metadata, String>() {
                                    override fun apply(input: MatchResult.Metadata): String {
                                        var fileName = input.resourceId().toString()
                                        return fileName.substring(
                                                0,
                                                fileName.lastIndexOf("/") + 1
                                        ) + "*"
                                    }
                                }
                        )
                )
                .apply(
                        "SearchForFiles",
                        FileIO.matchAll().withEmptyMatchTreatment(EmptyMatchTreatment.ALLOW)
                )
                .apply(
                        "ConvertToKVbyFolder",
                        ParDo.of(
                                object :
                                        DoFn<
                                                MatchResult.Metadata,
                                                KV<String, MatchResult.Metadata>>() {
                                    @ProcessElement
                                    fun processElement(
                                            @Element element: MatchResult.Metadata,
                                            receiver:
                                                    DoFn.OutputReceiver<
                                                            KV<String, MatchResult.Metadata>>
                                    ) {
                                        var folder = element.resourceId().toString()
                                        folder = folder.substring(0, folder.lastIndexOf("/") + 1)
                                        receiver.output(KV.of(folder, element))
                                    }
                                }
                        )
                )
                .apply("GroupByFolder", GroupByKey.create())
                .apply(
                        "FilterOutParsed",
                        ParDo.of(
                                object :
                                        DoFn<
                                                KV<
                                                        String,
                                                        Iterable<
                                                                @JvmWildcard MatchResult.Metadata>>,
                                                MatchResult.Metadata>() {
                                    @ProcessElement
                                    fun processElement(context: ProcessContext) {
                                        val element = context.element()
                                        var files = mutableMapOf<String, MatchResult.Metadata>()
                                        element.value.forEach {
                                            var filename = it.resourceId().toString()
                                            if (!filename.endsWith("_SUCCESS")) {

                                                filename = filename.removeSuffix(".parsed")
                                                if (files.containsKey(filename)) {
                                                    files.remove(filename)
                                                } else {
                                                    files[filename] = it
                                                }                                                
                                            }
                                        }
                                        files.forEach { context.output(it.value) }
                                    }
                                }
                        )
                )
    }
}

class S3ClientFactory : S3ClientBuilderFactory {

    companion object {
        val LOGGER = LoggerFactory.getLogger("s3tobigquery.S3ClientFactory")
    }

    fun getSecretValue(secret: String): String {
        var secretName = secret
        if (!secretName.startsWith("secret:")) {
            return secretName
        } else {
            secretName = secretName.removePrefix("secret:")
        }

        var client: SecretManagerServiceClient? = null
        try {
            client = SecretManagerServiceClient.create()
            val response = client.accessSecretVersion(secretName)
            return response.getPayload().getData().toStringUtf8()
        } catch (e: Exception) {
            LOGGER.error("Unable to read secret $secretName, error ", e)
        } finally {
            if (client != null) {
                client.close()
            }
        }
        return ""
    }

    override fun createBuilder(s3Options: S3Options): AmazonS3ClientBuilder {

        val options = s3Options.`as`(CustomPipelineOptions::class.java)
        val awsKey =
                if (options.awsAccessKeyId.isAccessible)
                        getSecretValue(options.awsAccessKeyId.get())
                else ""
        val awsSecret =
                if (options.awsSecretKey.isAccessible) getSecretValue(options.awsSecretKey.get())
                else ""

        var builder =
                AmazonS3ClientBuilder.standard()
                        .withCredentials(
                                AWSStaticCredentialsProvider(BasicAWSCredentials(awsKey, awsSecret))
                        )

        builder = builder.withRegion(s3Options.getAwsRegion())

        return builder
    }
}

fun main(args: Array<String>) {

    val options =
            PipelineOptionsFactory.fromArgs(*args)
                    .withValidation()
                    .`as`(CustomPipelineOptions::class.java)

    options.setS3ClientFactoryClass(S3ClientFactory::class.java)

    val tableSchemaJson = {}.javaClass.getResource("/schema.json").readText()

    val LOGGER = LoggerFactory.getLogger("s3tobigquery.AppKt")

    val p = Pipeline.create(options)

    p.getCoderRegistry()
            .registerCoderForClass(MatchResult.Metadata::class.java, MetadataCoderV2.of())

    val parsedResult =
            p.apply(
                            "GetCsvFiles",
                            GetCsvFiles(
                                    FolderListValueProvider(
                                            options.sourceBucket,
                                            options.startDate,
                                            options.lookBack
                                    )
                            )
                    )
                    .apply(
                            "ParseCsvFile",
                            ParDo.of(ReadCustomCsv())
                                    .withOutputTags(
                                            TABLE_ROW_TAG,
                                            TupleTagList.of(listOf(FILE_TAG))
                                    )
                    )

    parsedResult
            .get(FILE_TAG)
            .apply(
                    "WriteParsedFile",
                    MapElements.via(
                            object : SimpleFunction<String, String>() {
                                override fun apply(input: String): String {
                                    val filename = input.toString() + ".parsed"
                                    LOGGER.info("write file {}", filename)
                                    val newfile = FileSystems.matchNewResource(filename, false)
                                    FileSystems.create(newfile, "application/octet-stream").close()
                                    return filename
                                }
                            }
                    )
            )

    parsedResult
            .get(TABLE_ROW_TAG)
            .apply(
                    "WriteToBigQuery",
                    BigQueryIO.writeTableRows()
                            .to(options.outputTable)
                            .withJsonSchema(tableSchemaJson)
                            .withTimePartitioning(
                                    TimePartitioning().setField("receivetime").setType("DAY")
                            )
                            .withCreateDisposition(
                                    BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED
                            )
                            .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)
            )

    val result = p.run()
    if (!DataflowTemplateJob::class.isInstance(result)) result.waitUntilFinish()
}
